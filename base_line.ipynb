{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from typing import Optional, Callable\n",
    "import os\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.transforms import v2\n",
    "from torch.backends import cudnn\n",
    "from torch import GradScaler\n",
    "from torch import optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "pin_memory = True\n",
    "enable_half = True\n",
    "scaler = GradScaler(device, enabled=enable_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCachedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        # Runtime transforms are not implemented in this simple cached dataset.\n",
    "        self.data = tuple([x for x in dataset])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100_noisy_fine(Dataset):\n",
    "    \"\"\"\n",
    "    See https://github.com/UCSC-REAL/cifar-10-100n, https://www.noisylabels.com/ and `Learning with Noisy Labels\n",
    "    Revisited: A Study Using Real-World Human Annotations`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root: str, train: bool, transform: Optional[Callable], download: bool\n",
    "    ):\n",
    "        cifar100 = CIFAR100(\n",
    "            root=root, train=train, transform=transform, download=download\n",
    "        )\n",
    "        data, targets = tuple(zip(*cifar100))\n",
    "\n",
    "        if train:\n",
    "            noisy_label_file = os.path.join(root, \"CIFAR-100-noisy.npz\")\n",
    "            if not os.path.isfile(noisy_label_file):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"{type(self).__name__} need {noisy_label_file} to be used!\"\n",
    "                )\n",
    "\n",
    "            noise_file = np.load(noisy_label_file)\n",
    "            if not np.array_equal(noise_file[\"clean_label\"], targets):\n",
    "                raise RuntimeError(\"Clean labels do not match!\")\n",
    "            targets = noise_file[\"noisy_label\"]\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        return self.data[i], self.targets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25), inplace=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "CIFAR100_noisy_fine need data/CIFAR-100-noisy.npz to be used!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mCIFAR100_noisy_fine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasic_transforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_set \u001b[38;5;241m=\u001b[39m CIFAR100_noisy_fine(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mbasic_transforms)\n\u001b[1;32m      3\u001b[0m train_set \u001b[38;5;241m=\u001b[39m SimpleCachedDataset(train_set)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mCIFAR100_noisy_fine.__init__\u001b[0;34m(self, root, train, transform, download)\u001b[0m\n\u001b[1;32m     16\u001b[0m noisy_label_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR-100-noisy.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(noisy_label_file):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m need \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoisy_label_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be used!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     22\u001b[0m noise_file \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(noisy_label_file)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(noise_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_label\u001b[39m\u001b[38;5;124m\"\u001b[39m], targets):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: CIFAR100_noisy_fine need data/CIFAR-100-noisy.npz to be used!"
     ]
    }
   ],
   "source": [
    "train_set = CIFAR100_noisy_fine('data', download=True, train=True, transform=basic_transforms)\n",
    "test_set = CIFAR100_noisy_fine('data', download=True, train=False, transform=basic_transforms)\n",
    "train_set = SimpleCachedDataset(train_set)\n",
    "test_set = SimpleCachedDataset(test_set)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_set, batch_size=500, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"hf_hub:grodino/resnet18_cifar10\", pretrained=True)\n",
    "model.fc = nn.Linear(512, 100)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        with torch.autocast(device.type, enabled=enable_half):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predicted = outputs.argmax(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def val():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        with torch.autocast(device.type, enabled=enable_half):\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        predicted = outputs.argmax(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def inference():\n",
    "    model.eval()\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        with torch.autocast(device.type, enabled=enable_half):\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        predicted = outputs.argmax(1).tolist()\n",
    "        labels.extend(predicted)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 0.0\n",
    "epochs = list(range(10))\n",
    "with tqdm(epochs) as tbar:\n",
    "    for epoch in tbar:\n",
    "        train_acc = train()\n",
    "        val_acc = val()\n",
    "        if val_acc > best:\n",
    "            best = val_acc\n",
    "        tbar.set_description(f\"Train: {train_acc:.2f}, Val: {val_acc:.2f}, Best: {best:.2f}\")\n",
    "\n",
    "data = {\n",
    "    \"ID\": [],\n",
    "    \"target\": []\n",
    "}\n",
    "\n",
    "\n",
    "for i, label in enumerate(inference()):\n",
    "    data[\"ID\"].append(i)\n",
    "    data[\"target\"].append(label)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
