{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHpf_vbvLSPN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms.v2 import CutMix, MixUp\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR100N(Dataset):\n",
        "    def __init__(self, root, transform=None, noise_file='./drive/MyDrive/data/CIFAR-100_human.pt'):\n",
        "        self.cifar100 = CIFAR100(root=root, train=True, download=True, transform=transform)\n",
        "        noise_data = torch.load(noise_file)\n",
        "        self.labels = noise_data['noisy_label']\n",
        "    def __len__(self):\n",
        "        return len(self.cifar100)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, _ = self.cifar100[idx]\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "HSsPQZ6pLaHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def divide_mix(model, loader, threshold=0.8, device=\"cuda\"):\n",
        "    \"\"\"Splits the dataset into clean and noisy samples based on model confidence.\"\"\"\n",
        "    model.eval()\n",
        "    clean_indices = []\n",
        "    noisy_indices = []\n",
        "    confidences = []\n",
        "\n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = nn.functional.softmax(outputs, dim=1)\n",
        "        max_probs, preds = probs.max(dim=1)\n",
        "\n",
        "        for j in range(len(images)):\n",
        "            global_idx = i * loader.batch_size + j\n",
        "            if max_probs[j] > threshold:\n",
        "                clean_indices.append((global_idx, preds[j].item()))\n",
        "            else:\n",
        "                noisy_indices.append((global_idx, preds[j].item()))\n",
        "            confidences.append(max_probs[j].item())\n",
        "\n",
        "    print(f\"DivideMix Debug -> Clean: {len(clean_indices)}, Noisy: {len(noisy_indices)}\")\n",
        "\n",
        "    return clean_indices, noisy_indices, confidences"
      ],
      "metadata": {
        "id": "jGETX21hLrLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_with_dividemix(model, loader, optimizer, criterion, device, augmentation, clean_indices, noisy_indices):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if len(clean_indices) == 0:\n",
        "        print(\"No clean samples identified, skipping clean sample training.\")\n",
        "    if len(noisy_indices) == 0:\n",
        "        print(\"No noisy samples identified, skipping noisy sample training.\")\n",
        "\n",
        "    if len(clean_indices) > 0:\n",
        "        clean_loader = DataLoader(\n",
        "            Subset(loader.dataset, [idx for idx, _ in clean_indices]),\n",
        "            batch_size=loader.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=loader.num_workers\n",
        "        )\n",
        "        for images, labels in clean_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            images, labels = augmentation(images, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            labels_smooth = labels.argmax(dim=1) if labels.ndim > 1 else labels\n",
        "            correct += predicted.eq(labels_smooth).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    if len(noisy_indices) > 0:\n",
        "        noisy_loader = DataLoader(\n",
        "            Subset(loader.dataset, [idx for idx, _ in noisy_indices]),\n",
        "            batch_size=loader.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=loader.num_workers\n",
        "        )\n",
        "        for images, labels in noisy_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            images, labels = augmentation(images, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss = 0.5 * loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            labels_smooth = labels.argmax(dim=1) if labels.ndim > 1 else labels\n",
        "            correct += predicted.eq(labels_smooth).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, 100.0 * correct / total"
      ],
      "metadata": {
        "id": "slQ7WzRYLtzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        labels_smooth = labels.argmax(dim=1) if labels.ndim > 1 else labels\n",
        "        correct += predicted.eq(labels_smooth).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, 100.0 * correct / total\n"
      ],
      "metadata": {
        "id": "f94ci3BFLwtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.RandomCrop(224, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "    train_dataset = CIFAR100N(root='./drive/MyDrive/data', transform=transform, noise_file='./drive/MyDrive/data/CIFAR-100_human.pt')\n",
        "    test_dataset = CIFAR100(root='./drive/MyDrive/data', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True, num_workers=4,\n",
        "                              persistent_workers=True, prefetch_factor=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, pin_memory=True, num_workers=4,\n",
        "                             persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "    model = resnet18(weights=\"IMAGENET1K_V1\")\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "\n",
        "    # Acum antrenezi doar ultimul strat\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "    cutmix = CutMix(num_classes=100, alpha=1.0)\n",
        "    mixup = MixUp(num_classes=100, alpha=1.0)\n",
        "\n",
        "    num_epochs = 100\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        import time\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        clean_indices, noisy_indices, confidences = divide_mix(model, train_loader, threshold=0.8, device=device)\n",
        "        print(f\"Clean samples: {len(clean_indices)}, Noisy samples: {len(noisy_indices)}\")\n",
        "\n",
        "        augmentation = cutmix if np.random.rand() < 0.5 else mixup\n",
        "        train_loss, train_acc = train_epoch_with_dividemix(\n",
        "            model, train_loader, optimizer, criterion, device, augmentation, clean_indices, noisy_indices\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"Time taken: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_resnet_dividemix.pth')\n",
        "\n",
        "    print(f\"Best Validation Accuracy: {best_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0zBmzK1LxIS",
        "outputId": "9be6c63d-bda4-4ce7-e0f2-899b2e6be122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6d49dbc42abf>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  noise_data = torch.load(noise_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 200MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "DivideMix Debug -> Clean: 0, Noisy: 50000\n",
            "Clean samples: 0, Noisy samples: 50000\n",
            "No clean samples identified, skipping clean sample training.\n",
            "Train Loss: 2.0050, Train Acc: 16.08%, Val Loss: 2.9781, Val Acc: 32.09%\n",
            "Time taken: 72.58s\n",
            "Epoch 2/100\n",
            "DivideMix Debug -> Clean: 226, Noisy: 49774\n",
            "Clean samples: 226, Noisy samples: 49774\n",
            "Train Loss: 1.7835, Train Acc: 26.68%, Val Loss: 2.5690, Val Acc: 40.71%\n",
            "Time taken: 70.30s\n",
            "Epoch 3/100\n",
            "DivideMix Debug -> Clean: 758, Noisy: 49242\n",
            "Clean samples: 758, Noisy samples: 49242\n",
            "Train Loss: 1.8311, Train Acc: 23.13%, Val Loss: 2.4440, Val Acc: 43.08%\n",
            "Time taken: 70.07s\n",
            "Epoch 4/100\n",
            "DivideMix Debug -> Clean: 1290, Noisy: 48710\n",
            "Clean samples: 1290, Noisy samples: 48710\n",
            "Train Loss: 1.7429, Train Acc: 30.71%, Val Loss: 2.3536, Val Acc: 43.91%\n",
            "Time taken: 70.03s\n",
            "Epoch 5/100\n",
            "DivideMix Debug -> Clean: 1483, Noisy: 48517\n",
            "Clean samples: 1483, Noisy samples: 48517\n",
            "Train Loss: 1.7302, Train Acc: 31.89%, Val Loss: 2.2666, Val Acc: 46.82%\n",
            "Time taken: 70.41s\n",
            "Epoch 6/100\n",
            "DivideMix Debug -> Clean: 1635, Noisy: 48365\n",
            "Clean samples: 1635, Noisy samples: 48365\n",
            "Train Loss: 1.8256, Train Acc: 25.40%, Val Loss: 2.2687, Val Acc: 47.31%\n",
            "Time taken: 70.03s\n",
            "Epoch 7/100\n",
            "DivideMix Debug -> Clean: 1828, Noisy: 48172\n",
            "Clean samples: 1828, Noisy samples: 48172\n",
            "Train Loss: 1.8149, Train Acc: 25.90%, Val Loss: 2.2351, Val Acc: 47.20%\n",
            "Time taken: 69.59s\n",
            "Epoch 8/100\n",
            "DivideMix Debug -> Clean: 2244, Noisy: 47756\n",
            "Clean samples: 2244, Noisy samples: 47756\n",
            "Train Loss: 1.8126, Train Acc: 26.74%, Val Loss: 2.2122, Val Acc: 47.57%\n",
            "Time taken: 69.85s\n",
            "Epoch 9/100\n",
            "DivideMix Debug -> Clean: 2291, Noisy: 47709\n",
            "Clean samples: 2291, Noisy samples: 47709\n",
            "Train Loss: 1.7430, Train Acc: 32.93%, Val Loss: 2.2048, Val Acc: 47.76%\n",
            "Time taken: 70.36s\n",
            "Epoch 10/100\n",
            "DivideMix Debug -> Clean: 2079, Noisy: 47921\n",
            "Clean samples: 2079, Noisy samples: 47921\n",
            "Train Loss: 1.7363, Train Acc: 32.63%, Val Loss: 2.2048, Val Acc: 48.47%\n",
            "Time taken: 70.41s\n",
            "Epoch 11/100\n",
            "DivideMix Debug -> Clean: 2207, Noisy: 47793\n",
            "Clean samples: 2207, Noisy samples: 47793\n",
            "Train Loss: 1.8293, Train Acc: 25.80%, Val Loss: 2.1893, Val Acc: 49.21%\n",
            "Time taken: 69.82s\n",
            "Epoch 12/100\n",
            "DivideMix Debug -> Clean: 2321, Noisy: 47679\n",
            "Clean samples: 2321, Noisy samples: 47679\n",
            "Train Loss: 1.7393, Train Acc: 33.09%, Val Loss: 2.1387, Val Acc: 49.18%\n",
            "Time taken: 70.20s\n",
            "Epoch 13/100\n",
            "DivideMix Debug -> Clean: 2280, Noisy: 47720\n",
            "Clean samples: 2280, Noisy samples: 47720\n",
            "Train Loss: 1.7999, Train Acc: 27.48%, Val Loss: 2.1376, Val Acc: 49.79%\n",
            "Time taken: 70.00s\n",
            "Epoch 14/100\n",
            "DivideMix Debug -> Clean: 2561, Noisy: 47439\n",
            "Clean samples: 2561, Noisy samples: 47439\n",
            "Train Loss: 1.7719, Train Acc: 32.96%, Val Loss: 2.1102, Val Acc: 49.93%\n",
            "Time taken: 70.55s\n",
            "Epoch 15/100\n",
            "DivideMix Debug -> Clean: 2501, Noisy: 47499\n",
            "Clean samples: 2501, Noisy samples: 47499\n",
            "Train Loss: 1.8122, Train Acc: 27.85%, Val Loss: 2.1539, Val Acc: 50.03%\n",
            "Time taken: 69.89s\n",
            "Epoch 16/100\n",
            "DivideMix Debug -> Clean: 2492, Noisy: 47508\n",
            "Clean samples: 2492, Noisy samples: 47508\n",
            "Train Loss: 1.8240, Train Acc: 27.32%, Val Loss: 2.1396, Val Acc: 49.00%\n",
            "Time taken: 70.30s\n",
            "Epoch 17/100\n",
            "DivideMix Debug -> Clean: 2900, Noisy: 47100\n",
            "Clean samples: 2900, Noisy samples: 47100\n",
            "Train Loss: 1.7702, Train Acc: 33.15%, Val Loss: 2.1117, Val Acc: 50.11%\n",
            "Time taken: 70.36s\n",
            "Epoch 18/100\n",
            "DivideMix Debug -> Clean: 2452, Noisy: 47548\n",
            "Clean samples: 2452, Noisy samples: 47548\n",
            "Train Loss: 1.7358, Train Acc: 34.19%, Val Loss: 2.1765, Val Acc: 48.47%\n",
            "Time taken: 70.34s\n",
            "Epoch 19/100\n",
            "DivideMix Debug -> Clean: 2206, Noisy: 47794\n",
            "Clean samples: 2206, Noisy samples: 47794\n",
            "Train Loss: 1.7168, Train Acc: 33.88%, Val Loss: 2.1395, Val Acc: 50.06%\n",
            "Time taken: 70.42s\n",
            "Epoch 20/100\n",
            "DivideMix Debug -> Clean: 2121, Noisy: 47879\n",
            "Clean samples: 2121, Noisy samples: 47879\n",
            "Train Loss: 1.8051, Train Acc: 26.81%, Val Loss: 2.2008, Val Acc: 48.05%\n",
            "Time taken: 70.01s\n",
            "Epoch 21/100\n",
            "DivideMix Debug -> Clean: 2368, Noisy: 47632\n",
            "Clean samples: 2368, Noisy samples: 47632\n",
            "Train Loss: 1.7333, Train Acc: 33.86%, Val Loss: 2.1348, Val Acc: 49.12%\n",
            "Time taken: 70.54s\n",
            "Epoch 22/100\n",
            "DivideMix Debug -> Clean: 2453, Noisy: 47547\n",
            "Clean samples: 2453, Noisy samples: 47547\n",
            "Train Loss: 1.7326, Train Acc: 34.02%, Val Loss: 2.2125, Val Acc: 48.26%\n",
            "Time taken: 70.44s\n",
            "Epoch 23/100\n",
            "DivideMix Debug -> Clean: 1989, Noisy: 48011\n",
            "Clean samples: 1989, Noisy samples: 48011\n",
            "Train Loss: 1.8034, Train Acc: 27.55%, Val Loss: 2.1411, Val Acc: 49.33%\n",
            "Time taken: 70.08s\n",
            "Epoch 24/100\n",
            "DivideMix Debug -> Clean: 2524, Noisy: 47476\n",
            "Clean samples: 2524, Noisy samples: 47476\n",
            "Train Loss: 1.7428, Train Acc: 33.93%, Val Loss: 2.1463, Val Acc: 48.89%\n",
            "Time taken: 70.14s\n",
            "Epoch 25/100\n",
            "DivideMix Debug -> Clean: 2404, Noisy: 47596\n",
            "Clean samples: 2404, Noisy samples: 47596\n",
            "Train Loss: 1.8107, Train Acc: 27.61%, Val Loss: 2.1319, Val Acc: 48.86%\n",
            "Time taken: 69.75s\n",
            "Epoch 26/100\n",
            "DivideMix Debug -> Clean: 2567, Noisy: 47433\n",
            "Clean samples: 2567, Noisy samples: 47433\n",
            "Train Loss: 1.7454, Train Acc: 33.26%, Val Loss: 2.1704, Val Acc: 48.69%\n",
            "Time taken: 70.59s\n",
            "Epoch 27/100\n",
            "DivideMix Debug -> Clean: 2186, Noisy: 47814\n",
            "Clean samples: 2186, Noisy samples: 47814\n",
            "Train Loss: 1.7153, Train Acc: 35.02%, Val Loss: 2.1629, Val Acc: 49.01%\n",
            "Time taken: 70.33s\n",
            "Epoch 28/100\n",
            "DivideMix Debug -> Clean: 2171, Noisy: 47829\n",
            "Clean samples: 2171, Noisy samples: 47829\n",
            "Train Loss: 1.6987, Train Acc: 35.17%, Val Loss: 2.1211, Val Acc: 50.40%\n",
            "Time taken: 70.01s\n",
            "Epoch 29/100\n",
            "DivideMix Debug -> Clean: 2315, Noisy: 47685\n",
            "Clean samples: 2315, Noisy samples: 47685\n",
            "Train Loss: 1.8126, Train Acc: 27.94%, Val Loss: 2.1757, Val Acc: 49.27%\n",
            "Time taken: 70.05s\n",
            "Epoch 30/100\n",
            "DivideMix Debug -> Clean: 2246, Noisy: 47754\n",
            "Clean samples: 2246, Noisy samples: 47754\n",
            "Train Loss: 1.7268, Train Acc: 33.95%, Val Loss: 2.1134, Val Acc: 50.09%\n",
            "Time taken: 70.26s\n",
            "Epoch 31/100\n",
            "DivideMix Debug -> Clean: 2507, Noisy: 47493\n",
            "Clean samples: 2507, Noisy samples: 47493\n",
            "Train Loss: 1.8193, Train Acc: 26.86%, Val Loss: 2.1467, Val Acc: 50.05%\n",
            "Time taken: 69.73s\n",
            "Epoch 32/100\n",
            "DivideMix Debug -> Clean: 2460, Noisy: 47540\n",
            "Clean samples: 2460, Noisy samples: 47540\n",
            "Train Loss: 1.7974, Train Acc: 28.30%, Val Loss: 2.1054, Val Acc: 50.49%\n",
            "Time taken: 70.05s\n",
            "Epoch 33/100\n"
          ]
        }
      ]
    }
  ]
}