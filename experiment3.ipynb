{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GohYu68AAPs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms.v2 import CutMix, MixUp\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR100N(Dataset):\n",
        "    def __init__(self, root, transform=None, noise_file='./drive/MyDrive/data/CIFAR-100_human.pt'):\n",
        "        self.cifar100 = CIFAR100(root=root, train=True, download=True, transform=transform)\n",
        "        noise_data = torch.load(noise_file)\n",
        "        self.labels = noise_data['noisy_label']\n",
        "    def __len__(self):\n",
        "        return len(self.cifar100)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, _ = self.cifar100[idx]\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "BKdOeKIFABAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def divide_mix(model, loader, threshold=0.8, device=\"cuda\"):\n",
        "    \"\"\"Splits the dataset into clean and noisy samples based on model confidence.\"\"\"\n",
        "    model.eval()\n",
        "    clean_indices = []\n",
        "    noisy_indices = []\n",
        "    confidences = []\n",
        "\n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = nn.functional.softmax(outputs, dim=1)\n",
        "        max_probs, preds = probs.max(dim=1)\n",
        "\n",
        "        for j in range(len(images)):\n",
        "            global_idx = i * loader.batch_size + j\n",
        "            if max_probs[j] > threshold:\n",
        "                clean_indices.append((global_idx, preds[j].item()))\n",
        "            else:\n",
        "                noisy_indices.append((global_idx, preds[j].item()))\n",
        "            confidences.append(max_probs[j].item())\n",
        "\n",
        "    print(f\"DivideMix Debug -> Clean: {len(clean_indices)}, Noisy: {len(noisy_indices)}\")\n",
        "\n",
        "    return clean_indices, noisy_indices, confidences"
      ],
      "metadata": {
        "id": "HABQt_2uABVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_with_dividemix(model, loader, optimizer, criterion, device, augmentation, clean_indices, noisy_indices):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    if len(clean_indices) == 0:\n",
        "        print(\"No clean samples identified, skipping clean sample training.\")\n",
        "    if len(noisy_indices) == 0:\n",
        "        print(\"No noisy samples identified, skipping noisy sample training.\")\n",
        "\n",
        "    if len(clean_indices) > 0:\n",
        "        clean_loader = DataLoader(\n",
        "            Subset(loader.dataset, [idx for idx, _ in clean_indices]),\n",
        "            batch_size=loader.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=loader.num_workers\n",
        "        )\n",
        "        for images, labels in clean_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            images, labels = augmentation(images, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            labels_smooth = labels.argmax(dim=1) if labels.ndim > 1 else labels\n",
        "            correct += predicted.eq(labels_smooth).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    if len(noisy_indices) > 0:\n",
        "        noisy_loader = DataLoader(\n",
        "            Subset(loader.dataset, [idx for idx, _ in noisy_indices]),\n",
        "            batch_size=loader.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=loader.num_workers\n",
        "        )\n",
        "        for images, labels in noisy_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            images, labels = augmentation(images, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss = 0.5 * loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            labels_smooth = labels.argmax(dim=1) if labels.ndim > 1 else labels\n",
        "            correct += predicted.eq(labels_smooth).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, 100.0 * correct / total"
      ],
      "metadata": {
        "id": "MmS0mpvTABo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        labels_smooth = labels.argmax(dim=1) if labels.ndim > 1 else labels\n",
        "        correct += predicted.eq(labels_smooth).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, 100.0 * correct / total\n"
      ],
      "metadata": {
        "id": "_uZhPQXYAB8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.RandomCrop(224, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "    train_dataset = CIFAR100N(root='./drive/MyDrive/data', transform=transform, noise_file='./drive/MyDrive/data/CIFAR-100_human.pt')\n",
        "    test_dataset = CIFAR100(root='./drive/MyDrive/data', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True, num_workers=4,\n",
        "                              persistent_workers=True, prefetch_factor=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, pin_memory=True, num_workers=4,\n",
        "                             persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "    model = resnet18(weights=\"IMAGENET1K_V1\")\n",
        "    model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "    cutmix = CutMix(num_classes=100, alpha=1.0)\n",
        "    mixup = MixUp(num_classes=100, alpha=1.0)\n",
        "\n",
        "    num_epochs = 100\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        import time\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        clean_indices, noisy_indices, confidences = divide_mix(model, train_loader, threshold=0.8, device=device)\n",
        "        print(f\"Clean samples: {len(clean_indices)}, Noisy samples: {len(noisy_indices)}\")\n",
        "\n",
        "        augmentation = cutmix if np.random.rand() < 0.5 else mixup\n",
        "        train_loss, train_acc = train_epoch_with_dividemix(\n",
        "            model, train_loader, optimizer, criterion, device, augmentation, clean_indices, noisy_indices\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"Time taken: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_resnet_dividemix.pth')\n",
        "\n",
        "    print(f\"Best Validation Accuracy: {best_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ci-xnPDMAChA",
        "outputId": "e23172f8-e70e-4faf-8c32-4b006fe0ab7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6d49dbc42abf>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  noise_data = torch.load(noise_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 177MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "DivideMix Debug -> Clean: 0, Noisy: 50000\n",
            "Clean samples: 0, Noisy samples: 50000\n",
            "No clean samples identified, skipping clean sample training.\n",
            "Train Loss: 1.8131, Train Acc: 22.80%, Val Loss: 1.7658, Val Acc: 55.36%\n",
            "Time taken: 68.26s\n",
            "Epoch 2/100\n",
            "DivideMix Debug -> Clean: 5264, Noisy: 44736\n",
            "Clean samples: 5264, Noisy samples: 44736\n",
            "Train Loss: 1.7193, Train Acc: 36.62%, Val Loss: 1.5009, Val Acc: 62.91%\n",
            "Time taken: 66.33s\n",
            "Epoch 3/100\n",
            "DivideMix Debug -> Clean: 7704, Noisy: 42296\n",
            "Clean samples: 7704, Noisy samples: 42296\n",
            "Train Loss: 1.7391, Train Acc: 39.45%, Val Loss: 1.4196, Val Acc: 64.66%\n",
            "Time taken: 66.37s\n",
            "Epoch 4/100\n",
            "DivideMix Debug -> Clean: 8728, Noisy: 41272\n",
            "Clean samples: 8728, Noisy samples: 41272\n",
            "Train Loss: 1.7178, Train Acc: 41.85%, Val Loss: 1.3666, Val Acc: 65.77%\n",
            "Time taken: 66.26s\n",
            "Epoch 5/100\n",
            "DivideMix Debug -> Clean: 9602, Noisy: 40398\n",
            "Clean samples: 9602, Noisy samples: 40398\n",
            "Train Loss: 1.7602, Train Acc: 44.46%, Val Loss: 1.4679, Val Acc: 65.24%\n",
            "Time taken: 66.27s\n",
            "Epoch 6/100\n",
            "DivideMix Debug -> Clean: 6317, Noisy: 43683\n",
            "Clean samples: 6317, Noisy samples: 43683\n",
            "Train Loss: 1.6037, Train Acc: 42.33%, Val Loss: 1.3414, Val Acc: 67.17%\n",
            "Time taken: 66.46s\n",
            "Epoch 7/100\n",
            "DivideMix Debug -> Clean: 10194, Noisy: 39806\n",
            "Clean samples: 10194, Noisy samples: 39806\n",
            "Train Loss: 1.7026, Train Acc: 47.23%, Val Loss: 1.4420, Val Acc: 66.28%\n",
            "Time taken: 66.30s\n",
            "Epoch 8/100\n",
            "DivideMix Debug -> Clean: 7694, Noisy: 42306\n",
            "Clean samples: 7694, Noisy samples: 42306\n",
            "Train Loss: 1.6123, Train Acc: 48.31%, Val Loss: 1.3714, Val Acc: 67.47%\n",
            "Time taken: 66.14s\n",
            "Epoch 9/100\n",
            "DivideMix Debug -> Clean: 8521, Noisy: 41479\n",
            "Clean samples: 8521, Noisy samples: 41479\n",
            "Train Loss: 1.5728, Train Acc: 47.23%, Val Loss: 1.2741, Val Acc: 68.15%\n",
            "Time taken: 66.68s\n",
            "Epoch 10/100\n",
            "DivideMix Debug -> Clean: 12136, Noisy: 37864\n",
            "Clean samples: 12136, Noisy samples: 37864\n",
            "Train Loss: 1.6818, Train Acc: 46.48%, Val Loss: 1.2452, Val Acc: 69.06%\n",
            "Time taken: 66.65s\n",
            "Epoch 11/100\n",
            "DivideMix Debug -> Clean: 12798, Noisy: 37202\n",
            "Clean samples: 12798, Noisy samples: 37202\n",
            "Train Loss: 1.6841, Train Acc: 51.05%, Val Loss: 1.3897, Val Acc: 67.99%\n",
            "Time taken: 66.28s\n",
            "Epoch 12/100\n",
            "DivideMix Debug -> Clean: 8956, Noisy: 41044\n",
            "Clean samples: 8956, Noisy samples: 41044\n",
            "Train Loss: 1.5876, Train Acc: 47.89%, Val Loss: 1.2716, Val Acc: 68.76%\n",
            "Time taken: 66.32s\n",
            "Epoch 13/100\n",
            "DivideMix Debug -> Clean: 12560, Noisy: 37440\n",
            "Clean samples: 12560, Noisy samples: 37440\n",
            "Train Loss: 1.7063, Train Acc: 50.39%, Val Loss: 1.3727, Val Acc: 67.94%\n",
            "Time taken: 66.21s\n",
            "Epoch 14/100\n",
            "DivideMix Debug -> Clean: 9238, Noisy: 40762\n",
            "Clean samples: 9238, Noisy samples: 40762\n",
            "Train Loss: 1.5533, Train Acc: 52.33%, Val Loss: 1.4475, Val Acc: 67.08%\n",
            "Time taken: 66.61s\n",
            "Epoch 15/100\n",
            "DivideMix Debug -> Clean: 8080, Noisy: 41920\n",
            "Clean samples: 8080, Noisy samples: 41920\n",
            "Train Loss: 1.5176, Train Acc: 52.89%, Val Loss: 1.3596, Val Acc: 67.76%\n",
            "Time taken: 66.25s\n",
            "Epoch 16/100\n",
            "DivideMix Debug -> Clean: 10309, Noisy: 39691\n",
            "Clean samples: 10309, Noisy samples: 39691\n",
            "Train Loss: 1.5448, Train Acc: 54.03%, Val Loss: 1.4712, Val Acc: 66.19%\n",
            "Time taken: 66.54s\n",
            "Epoch 17/100\n",
            "DivideMix Debug -> Clean: 8261, Noisy: 41739\n",
            "Clean samples: 8261, Noisy samples: 41739\n",
            "Train Loss: 1.5163, Train Acc: 49.37%, Val Loss: 1.2784, Val Acc: 68.91%\n",
            "Time taken: 66.71s\n",
            "Epoch 18/100\n",
            "DivideMix Debug -> Clean: 14318, Noisy: 35682\n",
            "Clean samples: 14318, Noisy samples: 35682\n",
            "Train Loss: 1.6466, Train Acc: 54.00%, Val Loss: 1.4102, Val Acc: 67.44%\n",
            "Time taken: 66.76s\n",
            "Epoch 19/100\n",
            "DivideMix Debug -> Clean: 9776, Noisy: 40224\n",
            "Clean samples: 9776, Noisy samples: 40224\n",
            "Train Loss: 1.4961, Train Acc: 55.32%, Val Loss: 1.4298, Val Acc: 66.40%\n",
            "Time taken: 67.11s\n",
            "Epoch 20/100\n",
            "DivideMix Debug -> Clean: 10885, Noisy: 39115\n",
            "Clean samples: 10885, Noisy samples: 39115\n",
            "Train Loss: 1.5664, Train Acc: 50.78%, Val Loss: 1.2647, Val Acc: 69.18%\n",
            "Time taken: 66.45s\n",
            "Epoch 21/100\n",
            "DivideMix Debug -> Clean: 16620, Noisy: 33380\n",
            "Clean samples: 16620, Noisy samples: 33380\n",
            "Train Loss: 1.6715, Train Acc: 55.39%, Val Loss: 1.4523, Val Acc: 65.70%\n",
            "Time taken: 66.53s\n",
            "Epoch 22/100\n",
            "DivideMix Debug -> Clean: 11023, Noisy: 38977\n",
            "Clean samples: 11023, Noisy samples: 38977\n",
            "Train Loss: 1.5376, Train Acc: 50.93%, Val Loss: 1.2792, Val Acc: 67.93%\n",
            "Time taken: 66.43s\n",
            "Epoch 23/100\n",
            "DivideMix Debug -> Clean: 16886, Noisy: 33114\n",
            "Clean samples: 16886, Noisy samples: 33114\n",
            "Train Loss: 1.6707, Train Acc: 53.29%, Val Loss: 1.2962, Val Acc: 67.08%\n",
            "Time taken: 66.24s\n",
            "Epoch 24/100\n",
            "DivideMix Debug -> Clean: 17914, Noisy: 32086\n",
            "Clean samples: 17914, Noisy samples: 32086\n",
            "Train Loss: 1.6775, Train Acc: 53.02%, Val Loss: 1.2865, Val Acc: 68.05%\n",
            "Time taken: 66.21s\n",
            "Epoch 25/100\n",
            "DivideMix Debug -> Clean: 17796, Noisy: 32204\n",
            "Clean samples: 17796, Noisy samples: 32204\n",
            "Train Loss: 1.6851, Train Acc: 51.65%, Val Loss: 1.3340, Val Acc: 67.30%\n",
            "Time taken: 66.48s\n",
            "Epoch 26/100\n",
            "DivideMix Debug -> Clean: 16630, Noisy: 33370\n",
            "Clean samples: 16630, Noisy samples: 33370\n",
            "Train Loss: 1.6428, Train Acc: 57.31%, Val Loss: 1.4216, Val Acc: 65.64%\n",
            "Time taken: 66.68s\n",
            "Epoch 27/100\n",
            "DivideMix Debug -> Clean: 13471, Noisy: 36529\n",
            "Clean samples: 13471, Noisy samples: 36529\n",
            "Train Loss: 1.5284, Train Acc: 54.00%, Val Loss: 1.2985, Val Acc: 67.73%\n",
            "Time taken: 66.89s\n",
            "Epoch 28/100\n",
            "DivideMix Debug -> Clean: 18725, Noisy: 31275\n",
            "Clean samples: 18725, Noisy samples: 31275\n",
            "Train Loss: 1.6405, Train Acc: 54.93%, Val Loss: 1.3136, Val Acc: 66.95%\n",
            "Time taken: 66.54s\n",
            "Epoch 29/100\n",
            "DivideMix Debug -> Clean: 19302, Noisy: 30698\n",
            "Clean samples: 19302, Noisy samples: 30698\n",
            "Train Loss: 1.6649, Train Acc: 55.34%, Val Loss: 1.3620, Val Acc: 66.18%\n",
            "Time taken: 66.69s\n",
            "Epoch 30/100\n",
            "DivideMix Debug -> Clean: 18085, Noisy: 31915\n",
            "Clean samples: 18085, Noisy samples: 31915\n",
            "Train Loss: 1.6155, Train Acc: 55.92%, Val Loss: 1.3235, Val Acc: 66.74%\n",
            "Time taken: 66.41s\n",
            "Epoch 31/100\n",
            "DivideMix Debug -> Clean: 20064, Noisy: 29936\n",
            "Clean samples: 20064, Noisy samples: 29936\n",
            "Train Loss: 1.6677, Train Acc: 55.41%, Val Loss: 1.3883, Val Acc: 66.23%\n",
            "Time taken: 66.56s\n",
            "Epoch 32/100\n",
            "DivideMix Debug -> Clean: 17694, Noisy: 32306\n",
            "Clean samples: 17694, Noisy samples: 32306\n",
            "Train Loss: 1.6304, Train Acc: 59.61%, Val Loss: 1.5276, Val Acc: 63.85%\n",
            "Time taken: 66.66s\n",
            "Epoch 33/100\n",
            "DivideMix Debug -> Clean: 13970, Noisy: 36030\n",
            "Clean samples: 13970, Noisy samples: 36030\n",
            "Train Loss: 1.5488, Train Acc: 60.10%, Val Loss: 1.4634, Val Acc: 64.73%\n",
            "Time taken: 66.42s\n",
            "Epoch 34/100\n",
            "DivideMix Debug -> Clean: 16030, Noisy: 33970\n",
            "Clean samples: 16030, Noisy samples: 33970\n",
            "Train Loss: 1.5014, Train Acc: 63.27%, Val Loss: 1.5079, Val Acc: 63.55%\n",
            "Time taken: 66.65s\n",
            "Epoch 35/100\n",
            "DivideMix Debug -> Clean: 16448, Noisy: 33552\n",
            "Clean samples: 16448, Noisy samples: 33552\n",
            "Train Loss: 1.5173, Train Acc: 59.01%, Val Loss: 1.3366, Val Acc: 66.15%\n",
            "Time taken: 66.31s\n",
            "Epoch 36/100\n",
            "DivideMix Debug -> Clean: 22681, Noisy: 27319\n",
            "Clean samples: 22681, Noisy samples: 27319\n",
            "Train Loss: 1.6754, Train Acc: 63.66%, Val Loss: 1.5135, Val Acc: 63.76%\n",
            "Time taken: 66.59s\n",
            "Epoch 37/100\n",
            "DivideMix Debug -> Clean: 17250, Noisy: 32750\n",
            "Clean samples: 17250, Noisy samples: 32750\n",
            "Train Loss: 1.5115, Train Acc: 60.39%, Val Loss: 1.3887, Val Acc: 65.33%\n",
            "Time taken: 66.67s\n",
            "Epoch 38/100\n",
            "DivideMix Debug -> Clean: 22971, Noisy: 27029\n",
            "Clean samples: 22971, Noisy samples: 27029\n",
            "Train Loss: 1.6383, Train Acc: 65.27%, Val Loss: 1.6024, Val Acc: 62.28%\n",
            "Time taken: 66.85s\n",
            "Epoch 39/100\n",
            "DivideMix Debug -> Clean: 15039, Noisy: 34961\n",
            "Clean samples: 15039, Noisy samples: 34961\n",
            "Train Loss: 1.4367, Train Acc: 61.03%, Val Loss: 1.4059, Val Acc: 65.27%\n",
            "Time taken: 67.40s\n",
            "Epoch 40/100\n",
            "DivideMix Debug -> Clean: 24250, Noisy: 25750\n",
            "Clean samples: 24250, Noisy samples: 25750\n",
            "Train Loss: 1.6240, Train Acc: 61.39%, Val Loss: 1.3999, Val Acc: 64.81%\n",
            "Time taken: 66.63s\n",
            "Epoch 41/100\n",
            "DivideMix Debug -> Clean: 25619, Noisy: 24381\n",
            "Clean samples: 25619, Noisy samples: 24381\n",
            "Train Loss: 1.6943, Train Acc: 65.41%, Val Loss: 1.5844, Val Acc: 62.15%\n",
            "Time taken: 66.72s\n",
            "Epoch 42/100\n",
            "DivideMix Debug -> Clean: 18354, Noisy: 31646\n",
            "Clean samples: 18354, Noisy samples: 31646\n",
            "Train Loss: 1.5179, Train Acc: 66.31%, Val Loss: 1.6108, Val Acc: 61.53%\n",
            "Time taken: 66.52s\n",
            "Epoch 43/100\n",
            "DivideMix Debug -> Clean: 18055, Noisy: 31945\n",
            "Clean samples: 18055, Noisy samples: 31945\n",
            "Train Loss: 1.4451, Train Acc: 68.52%, Val Loss: 1.5096, Val Acc: 62.89%\n",
            "Time taken: 66.90s\n",
            "Epoch 44/100\n",
            "DivideMix Debug -> Clean: 23992, Noisy: 26008\n",
            "Clean samples: 23992, Noisy samples: 26008\n",
            "Train Loss: 1.5942, Train Acc: 69.08%, Val Loss: 1.6002, Val Acc: 61.86%\n",
            "Time taken: 66.83s\n",
            "Epoch 45/100\n",
            "DivideMix Debug -> Clean: 19885, Noisy: 30115\n",
            "Clean samples: 19885, Noisy samples: 30115\n",
            "Train Loss: 1.4731, Train Acc: 70.51%, Val Loss: 1.5831, Val Acc: 62.27%\n",
            "Time taken: 66.77s\n",
            "Epoch 46/100\n",
            "DivideMix Debug -> Clean: 21015, Noisy: 28985\n",
            "Clean samples: 21015, Noisy samples: 28985\n",
            "Train Loss: 1.4397, Train Acc: 72.25%, Val Loss: 1.5958, Val Acc: 62.03%\n",
            "Time taken: 66.61s\n",
            "Epoch 47/100\n",
            "DivideMix Debug -> Clean: 22989, Noisy: 27011\n",
            "Clean samples: 22989, Noisy samples: 27011\n",
            "Train Loss: 1.4731, Train Acc: 72.45%, Val Loss: 1.6137, Val Acc: 61.44%\n",
            "Time taken: 66.73s\n",
            "Epoch 48/100\n",
            "DivideMix Debug -> Clean: 22098, Noisy: 27902\n",
            "Clean samples: 22098, Noisy samples: 27902\n",
            "Train Loss: 1.5097, Train Acc: 70.99%, Val Loss: 1.6128, Val Acc: 61.70%\n",
            "Time taken: 66.65s\n",
            "Epoch 49/100\n",
            "DivideMix Debug -> Clean: 21970, Noisy: 28030\n",
            "Clean samples: 21970, Noisy samples: 28030\n",
            "Train Loss: 1.5135, Train Acc: 65.56%, Val Loss: 1.4154, Val Acc: 64.48%\n",
            "Time taken: 66.91s\n",
            "Epoch 50/100\n",
            "DivideMix Debug -> Clean: 31250, Noisy: 18750\n",
            "Clean samples: 31250, Noisy samples: 18750\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-03374090d3aa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0maugmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcutmix\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmixup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         train_loss, train_acc = train_epoch_with_dividemix(\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoisy_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-4-854df67ca0e5>\u001b[0m in \u001b[0;36mtrain_epoch_with_dividemix\u001b[0;34m(model, loader, optimizer, criterion, device, augmentation, clean_indices, noisy_indices)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlabels_smooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}